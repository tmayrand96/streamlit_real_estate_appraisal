# Streamlit Property Valuation App with Multi-Region Quantile Regression
import streamlit as st
import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_absolute_error
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import shap
import warnings
import unicodedata
import re
warnings.filterwarnings('ignore')

# ---------------- CONFIG ----------------
# Base directory of the current script
BASE_DIR = Path(__file__).parent
DATA_DIR   = BASE_DIR / "data"     # <‚Äî new
MODELS_DIR = BASE_DIR / "models"   # <‚Äî optional but recommended

# ---- Minimal target handling (simple & strict) ----
CANON_TARGET = "Prix_de_vente"

def load_region_dataframe_simple(region_key: str) -> pd.DataFrame:
    """Load the CSV for a region and make sure the target is named Prix_de_vente."""
    cfg = REGION_CONFIG[region_key]
    df = pd.read_csv(cfg["data_path"], encoding="utf-8-sig")

    # normalize trivial header differences
    df.columns = [c.strip() for c in df.columns]

    # minimal target handling: accept a few common variants, otherwise fail fast
    if CANON_TARGET not in df.columns:
        for alt in ["Prix_de_Vente", "prix_de_vente", "Prix", "Price", "price_sold"]:
            if alt in df.columns:
                df = df.rename(columns={alt: CANON_TARGET})
                break

    if CANON_TARGET not in df.columns:
        raise ValueError(
            f"[{region_key}] Target column '{CANON_TARGET}' not found in {cfg['data_path'].name}. "
            f"Found columns: {list(df.columns)}. "
            "Rename your CSV header to 'Prix_de_vente' (preferred), "
            "or add its current name to the alt list above."
        )

    return df

# Multi-region configuration
REGION_CONFIG = {
    "BDF": {
        "name": "Bois-Des-Filion",
        "data_path": DATA_DIR / "donnees_BDF.csv",
        "feature_cols": ["Etage", "Age", "Aire_Batiment", "Aire_Lot", "Prox_Riverain"],
        "num_cols": ["Etage", "Age", "Aire_Batiment", "Aire_Lot"],
        "cat_cols": [],
        "model_prefix": "bdf"
    },
    "PMR": {
        "name": "Plateau Mont-Royal",
        "data_path": DATA_DIR / "Dataset_PMR.csv",
        "feature_cols": ["CONDO", "5PLEX_ET_MOINS", "6PLEX_ET_PLUS", "UNIFAMILIALE", "ETAGES", "AGE", "AIRE_HABITABLE", "TAXES_AN", "Prox_Parc", "Prox_Metro"],
        "num_cols": ["ETAGES", "AGE", "AIRE_HABITABLE", "TAXES_AN", "Prox_Parc", "Prox_Metro"],
        "cat_cols": ["CONDO", "5PLEX_ET_MOINS", "6PLEX_ET_PLUS", "UNIFAMILIALE"],
        "model_prefix": "pmr"
    },
    "Ste-Rose": {
        "name": "Sainte-Rose",
        "data_path": DATA_DIR / "Dataset_Ste-Rose.csv",
        "feature_cols": ["Etage", "Age", "Aire_Batiment_m2", "Aire_Lot_m2", "Garage", "Amenagement_paysager"],
        "num_cols": ["Etage", "Age", "Aire_Batiment_m2", "Aire_Lot_m2", "Garage", "Amenagement_paysager"],
        "cat_cols": [],
        "model_prefix": "ste_rose"
    }
}


# Create models directory if it doesn't exist
MODELS_DIR = BASE_DIR / "models"
MODELS_DIR.mkdir(exist_ok=True)

def model_path_for(region_key, alpha):
    """Get model path for a specific region and quantile"""
    prefix = REGION_CONFIG[region_key]["model_prefix"]
    q = {0.05: "q05", 0.5: "q50", 0.95: "q95"}[alpha]
    return MODELS_DIR / f"{prefix}_model_{q}.joblib"

# Legacy compatibility - load BDF models if they exist
try:
    if (BASE_DIR / "property_model_q05.joblib").exists():
        # Migrate old models to new structure if needed
        pass
except:
    pass


# Page configuration
st.set_page_config(
    page_title="Property Valuation System - Quantile Regression",
    page_icon="üè†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .prediction-card {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 2rem;
        border-radius: 1rem;
        text-align: center;
        margin: 1rem 0;
    }
    .quantile-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1.5rem;
        border-radius: 0.8rem;
        text-align: center;
        margin: 0.5rem 0;
    }
    .info-box {
        background-color: #e8f4fd;
        border: 1px solid #bee5eb;
        border-radius: 0.5rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.5rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 0.5rem;
        padding: 1rem;
        margin: 1rem 0;
    }
    .stButton > button {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 0.5rem;
        padding: 0.5rem 2rem;
        font-weight: bold;
    }
    .stButton > button:hover {
        background: linear-gradient(90deg, #5a6fd8 0%, #6a4190 100%);
        transform: translateY(-2px);
        transition: all 0.3s ease;
    }
    /* Compact SHAP analysis styles with theme-consistent colors */
    .shap-card-grid {
        display: flex;
        gap: 0.5rem;
        flex-wrap: nowrap;
        overflow-x: auto;
        padding-bottom: 0.25rem;
        margin: 0.25rem 0 0.5rem 0;
    }
    .shap-card {
        border-radius: 0.5rem;
        padding: 0.5rem 0.75rem;
        min-width: 220px;
        box-shadow: none;
        border: 1px solid transparent;
        transition: all 0.2s ease;
    }
    .shap-card h5 {
        margin: 0 0 0.25rem 0;
        font-size: 0.9rem;
        color: #334155;
        font-weight: 600;
    }
    .shap-card .value {
        font-weight: 600;
        font-size: 1rem;
        margin: 0 0 0.25rem 0;
    }
    .shap-card .desc {
        font-size: 0.85rem;
        color: #4b5563;
        margin: 0;
    }
    
    /* Theme-consistent color palette for SHAP cards */
    .shap-card.building-efficiency {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
    }
    .shap-card.building-efficiency h5 { color: #f0f2f6; }
    .shap-card.building-efficiency .desc { color: #e8f4fd; }
    .shap-card.building-efficiency.positive .value { color: #ffffff; }
    .shap-card.building-efficiency.negative .value { color: #ffebee; }
    
    .shap-card.condition {
        background: linear-gradient(135deg, #1f77b4 0%, #4a90e2 100%);
        color: white;
    }
    .shap-card.condition h5 { color: #f0f2f6; }
    .shap-card.condition .desc { color: #e8f4fd; }
    .shap-card.condition.positive .value { color: #ffffff; }
    .shap-card.condition.negative .value { color: #ffebee; }
    
    .shap-card.premium-location {
        background: linear-gradient(135deg, #5a6fd8 0%, #8b5cf6 100%);
        color: white;
    }
    .shap-card.premium-location h5 { color: #f0f2f6; }
    .shap-card.premium-location .desc { color: #e8f4fd; }
    .shap-card.premium-location.positive .value { color: #ffffff; }
    .shap-card.premium-location.negative .value { color: #ffebee; }
    
    .shap-card.floor-level {
        background: linear-gradient(135deg, #64748b 0%, #94a3b8 100%);
        color: white;
    }
    .shap-card.floor-level h5 { color: #f0f2f6; }
    .shap-card.floor-level .desc { color: #e8f4fd; }
    .shap-card.floor-level.positive .value { color: #ffffff; }
    .shap-card.floor-level.negative .value { color: #ffebee; }
    
    .shap-card.lot-size {
        background: linear-gradient(135deg, #6b7280 0%, #9ca3af 100%);
        color: white;
    }
    .shap-card.lot-size h5 { color: #f0f2f6; }
    .shap-card.lot-size .desc { color: #e8f4fd; }
    .shap-card.lot-size.positive .value { color: #ffffff; }
    .shap-card.lot-size.negative .value { color: #ffebee; }
    
    .compact-info { margin: 0.25rem 0; font-size: 0.85rem; }
    .scroll-x { overflow-x: auto; }
</style>
""", unsafe_allow_html=True)

# ---------------- FONCTIONS ----------------
def create_features(df, region_key="BDF", is_training=True):
    """Prepare raw data features for a specific region"""
    df = df.copy()
    
    # Get region configuration
    config = REGION_CONFIG[region_key]
    feature_cols = config["feature_cols"]
    num_cols = config["num_cols"]
    cat_cols = config["cat_cols"]
    
    # Auto-detect available columns and adapt feature_cols if needed
    available_cols = set(df.columns)
    adapted_feature_cols = [col for col in feature_cols if col in available_cols]
    
    if len(adapted_feature_cols) != len(feature_cols):
        st.warning(f"Some expected columns missing for {region_key}. Using available: {adapted_feature_cols}")
    
    # Ensure all required columns exist with sensible defaults
    for col in adapted_feature_cols:
        if col not in df.columns:
            if col in num_cols:
                df[col] = 0  # Default numeric value
            elif col in cat_cols:
                df[col] = "__missing__"  # Default categorical value
            else:
                df[col] = 0  # Default fallback
    
    # Handle PMR-specific boolean conversions
    if region_key == "PMR":
        # Convert boolean text columns to 0/1 if needed
        for col in ["Near_A_Park", "Near_Metro_Station"]:
            if col in df.columns:
                if df[col].dtype == 'object':
                    df[col] = df[col].map({'Yes': 1, 'No': 0, 'True': 1, 'False': 0, True: 1, False: 0}).fillna(0)
    
    if not is_training:
        df = df.fillna(0)
    
    # Return only the features needed by this region
    return df[adapted_feature_cols]

def train_quantile_models(region_key="BDF"):
    """Train quantile regression models for a specific region using sklearn Pipeline"""
    config = REGION_CONFIG[region_key]
    csv_path = config["data_path"]

    if not csv_path.exists():
        raise FileNotFoundError(f"Dataset introuvable : {csv_path}")

    with st.spinner(f"Loading and preparing data for {config['name']}..."):
        # If you added the simple loader, call it here, else keep pd.read_csv(csv_path)
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
        # If needed, normalize the target a tiny bit:
        if "Prix_de_vente" not in df.columns:
            for alt in ["Prix_de_Vente", "prix_de_vente", "Prix", "Price", "price_sold"]:
                if alt in df.columns:
                    df = df.rename(columns={alt: "Prix_de_vente"})
                    break

        df = create_features(df, region_key, is_training=True)

        if "Prix_de_vente" not in df.columns:
            raise ValueError(f"[{config['name']}] Target 'Prix_de_vente' not found. Columns: {list(df.columns)}")

        df = df.dropna(subset=['Prix_de_vente'])

        # Build feature list strictly from your REGION_CONFIG
        feature_cols = [c for c in config["feature_cols"] if c in df.columns]
        if not feature_cols:
            raise ValueError(f"[{config['name']}] No usable features. Columns present: {list(df.columns)}")

        X = df[feature_cols]
        y = df['Prix_de_vente'].astype(float)

    # Build preprocessing pipeline
    num_cols = [col for col in config["num_cols"] if col in feature_cols]
    cat_cols = [col for col in config["cat_cols"] if col in feature_cols]

    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', RobustScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='__missing__')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, num_cols),
            ('cat', categorical_transformer, cat_cols)
        ],
        remainder='drop'
    )

    metrics = {}
    with st.spinner("Training quantile models."):
        for alpha in [0.05, 0.50, 0.95]:
            pipe = Pipeline([
                ('preproc', preprocessor),
                ('model', GradientBoostingRegressor(loss="quantile", alpha=alpha, random_state=42))
            ])

            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
            pipe.fit(X_train, y_train)

            model_data = {
                "pipeline": pipe,
                "features": feature_cols,
                "region": region_key,
                "num_cols": num_cols,
                "cat_cols": cat_cols
            }

            if alpha == 0.50:
                y_pred_val = pipe.predict(X_val)
                metrics["R2"] = r2_score(y_val, y_pred_val)
                metrics["MAE"] = mean_absolute_error(y_val, y_pred_val)
                model_data["mae"] = metrics["MAE"]

            path = model_path_for(region_key, alpha)
            joblib.dump(model_data, path)

    return metrics

def predict_with_models(region_key="BDF", **kwargs):
    """Make predictions using all three quantile models for a specific region"""
    # Prepare inputs based on region
    config = REGION_CONFIG[region_key]
    feature_cols = config["feature_cols"]
    
    # Create input DataFrame with all possible features
    inputs_dict = {}
    for col in feature_cols:
        if col in kwargs:
            inputs_dict[col] = kwargs[col]
        else:
            # Default values for missing features
            if col in config["num_cols"]:
                inputs_dict[col] = 0
            elif col in config["cat_cols"]:
                inputs_dict[col] = "__missing__"
            else:
                inputs_dict[col] = 0
    
    inputs = pd.DataFrame([inputs_dict])
    inputs = create_features(inputs, region_key, is_training=False)

    preds = {}
    for alpha in [0.05, 0.50, 0.95]:
        path = model_path_for(region_key, alpha)
        try:
            data = joblib.load(path)
            pipe = data["pipeline"]
            preds[alpha] = float(pipe.predict(inputs)[0])
        except FileNotFoundError:
            st.error(f"Model not found for {region_key} quantile {alpha}. Please train models first.")
            return None, None, None

    return preds[0.05], preds[0.50], preds[0.95]

def get_model_mae(region_key="BDF"):
    """Get the MAE from the trained model for a specific region"""
    try:
        # Load the median model to get the MAE
        path = model_path_for(region_key, 0.5)
        data = joblib.load(path)
        
        if "mae" in data:
            return data["mae"]
        else:
            # If MAE is not stored in the model, compute it from the training data
            config = REGION_CONFIG[region_key]
            df = pd.read_csv(config["data_path"])
            df = create_features(df, region_key, is_training=True)
            df = df.dropna(subset=['Prix_de_vente'])
            
            feature_cols = data["features"]
            X = df[feature_cols]
            y = df['Prix_de_vente']
            
            pipe = data["pipeline"]
            y_pred = pipe.predict(X)
            
            return mean_absolute_error(y, y_pred)
    except Exception as e:
        st.warning(f"Could not retrieve MAE for {region_key}: {e}")
        return None

def create_shap_waterfall_chart(region_key="BDF", predicted_value=None, **kwargs):
    """
    Create a SHAP waterfall chart showing how the predicted price is constructed.
    """
    try:
        # Load the median model (q50)
        path = model_path_for(region_key, 0.5)
        data = joblib.load(path)
        pipe = data["pipeline"]
        features = data["features"]

        # Prepare single-row input as used by the model
        config = REGION_CONFIG[region_key]
        inputs_dict = {}
        for col in config["feature_cols"]:
            if col in kwargs:
                inputs_dict[col] = kwargs[col]
            else:
                if col in config["num_cols"]:
                    inputs_dict[col] = 0
                elif col in config["cat_cols"]:
                    inputs_dict[col] = "__missing__"
                else:
                    inputs_dict[col] = 0

        inputs = pd.DataFrame([inputs_dict])
        inputs = create_features(inputs, region_key, is_training=False)

        # Transform input and sample a background set in model's feature space
        X_transformed = pipe.named_steps["preproc"].transform(inputs)

        df_bg = pd.read_csv(config["data_path"])
        df_bg = create_features(df_bg, region_key, is_training=True).dropna(subset=['Prix_de_vente'])
        background_sample = df_bg[features].sample(min(100, len(df_bg)), random_state=42)
        X_background = pipe.named_steps["preproc"].transform(background_sample)

        # SHAP for GradientBoostingRegressor
        model = pipe.named_steps["model"]
        explainer = shap.TreeExplainer(model, data=X_background)
        shap_values = explainer.shap_values(X_transformed)  # shape: (1, n_features)
        base_value = float(explainer.expected_value)

        # Build waterfall data
        feature_names = features
        shap_contributions = shap_values[0]  # (n_features,)
        waterfall_data = []
        cumulative_value = base_value

        # Base
        waterfall_data.append({
            'feature': 'Base Value',
            'contribution': 0.0,
            'cumulative': base_value,
            'color': '#1f77b4'
        })

        # Contributions
        for feature, contribution in zip(feature_names, shap_contributions):
            c = float(contribution)
            cumulative_value += c
            color = '#2ca02c' if c >= 0 else '#d62728'
            waterfall_data.append({
                'feature': feature,
                'contribution': c,
                'cumulative': cumulative_value,
                'color': color
            })

        # Final prediction
        final_val = float(predicted_value) if predicted_value is not None else cumulative_value
        waterfall_data.append({
            'feature': 'Final Prediction',
            'contribution': 0.0,
            'cumulative': final_val,
            'color': '#1f77b4'
        })

        # Plotly figure
        fig = go.Figure()

        for i, point in enumerate(waterfall_data):
            if i == 0 or i == len(waterfall_data) - 1:
                # Base and Final as absolute bars
                cumulative_val = float(point['cumulative'])
                fig.add_trace(go.Bar(
                    x=[point['feature']],
                    y=[cumulative_val],
                    marker_color=point['color'],
                    name=point['feature'],
                    text=[f"${cumulative_val:,.0f}"],
                    textposition='auto',
                    showlegend=False
                ))
            else:
                # Contributions as delta bars
                contribution_val = float(point['contribution'])
                fig.add_trace(go.Bar(
                    x=[point['feature']],
                    y=[contribution_val],
                    marker_color=point['color'],
                    name=point['feature'],
                    text=[f"${contribution_val:,.0f}"],
                    textposition='auto',
                    showlegend=False
                ))

        # Flow line
        x_positions = list(range(len(waterfall_data)))
        cumulative_values = [float(pt['cumulative']) for pt in waterfall_data]
        fig.add_trace(go.Scatter(
            x=x_positions,
            y=cumulative_values,
            mode='lines+markers',
            line=dict(color='#666666', width=2, dash='dot'),
            marker=dict(size=6, color='#666666'),
            name='Flow',
            showlegend=False,
            hoverinfo='skip'
        ))

        fig.update_layout(
            title="Price Construction (SHAP Waterfall)",
            xaxis_title="Features",
            yaxis_title="Price ($)",
            height=500,
            showlegend=False,
            xaxis=dict(
                tickangle=45,
                tickmode='array',
                tickvals=list(range(len(waterfall_data))),
                ticktext=[pt['feature'] for pt in waterfall_data]
            ),
            yaxis=dict(tickformat='$,.0f'),
            margin=dict(b=100)
        )
        return fig

    except Exception as e:
        st.warning(f"Could not create SHAP waterfall chart: {e}")
        return None


def find_nearest_neighbors_and_calculate_ratio(region_key="BDF", predicted_value=None, k=3, **kwargs):
    """
    Find nearest neighbors in the dataset and calculate the ratio between predicted and actual prices.
    
    Args:
        region_key: Region identifier
        predicted_value: The predicted value from the model
        k: Number of nearest neighbors to consider (default=3)
        **kwargs: Property features
        
    Returns:
        dict: Contains ratio, actual_price, and whether averaging was used
    """
    try:
        # Load the dataset
        config = REGION_CONFIG[region_key]
        df = pd.read_csv(config["data_path"])
        df = create_features(df, region_key, is_training=True)
        df = df.dropna(subset=['Prix_de_vente'])
        
        # Small data safeguards
        n_samples = len(df)
        k = min(k, n_samples - 1)  # Ensure k doesn't exceed available samples
        
        # Load the pipeline for preprocessing
        path = model_path_for(region_key, 0.5)
        data = joblib.load(path)
        pipe = data["pipeline"]
        features = data["features"]
        
        # Prepare input data
        inputs_dict = {}
        for col in config["feature_cols"]:
            if col in kwargs:
                inputs_dict[col] = kwargs[col]
            else:
                if col in config["num_cols"]:
                    inputs_dict[col] = 0
                elif col in config["cat_cols"]:
                    inputs_dict[col] = "__missing__"
                else:
                    inputs_dict[col] = 0
        
        inputs = pd.DataFrame([inputs_dict])
        inputs = create_features(inputs, region_key, is_training=False)
        
        # Transform using pipeline preprocessor
        X_transformed = pipe.named_steps["preproc"].transform(inputs)
        
        # Transform all dataset features
        X_dataset = pipe.named_steps["preproc"].transform(df[features])
        
        # Check for exact match first in original feature space
        exact_match = df.copy()
        for col in config["feature_cols"]:
            if col in kwargs:
                exact_match = exact_match[exact_match[col] == kwargs[col]]
        
        if len(exact_match) > 0:
            # Use the first exact match
            actual_price = exact_match['Prix_de_vente'].iloc[0]
            used_averaging = False
        else:
            # Find k nearest neighbors using Euclidean distance in preprocessed space
            distances = np.sqrt(np.sum((X_dataset - X_transformed) ** 2, axis=1))
            
            # Get indices of k nearest neighbors
            nearest_indices = np.argsort(distances)[:k]
            
            # Get actual prices of nearest neighbors
            actual_prices = df.iloc[nearest_indices]['Prix_de_vente'].values
            
            # Use average of k nearest neighbors
            actual_price = np.mean(actual_prices)
            used_averaging = True
        
        # Calculate ratio
        ratio = (predicted_value / actual_price) * 100
        
        return {
            'ratio': ratio,
            'actual_price': actual_price,
            'used_averaging': used_averaging,
            'k': k
        }
        
    except Exception as e:
        st.warning(f"Could not calculate ratio for {region_key}: {e}")
        return None

def compute_shap_values(region_key="BDF", **kwargs):
    """
    Compute SHAP values for property prediction using the trained GradientBoostingRegressor model.
    Returns a dict keyed by original feature name.
    """
    try:
        path = model_path_for(region_key, 0.5)
        data = joblib.load(path)
        pipe = data["pipeline"]
        features = data["features"]

        # Build input row
        config = REGION_CONFIG[region_key]
        inputs_dict = {}
        for col in config["feature_cols"]:
            if col in kwargs:
                inputs_dict[col] = kwargs[col]
            else:
                if col in config["num_cols"]:
                    inputs_dict[col] = 0
                elif col in config["cat_cols"]:
                    inputs_dict[col] = "__missing__"
                else:
                    inputs_dict[col] = 0

        # Keep some raw values for explanations
        aire_batiment = float(inputs_dict.get("Aire_Batiment", 0) or 0)
        aire_lot = float(inputs_dict.get("Aire_Lot", 0) or 0)
        age = float(inputs_dict.get("Age", 0) or 0)
        etage = float(inputs_dict.get("Etage", 0) or 0)
        prox_riverain = int(inputs_dict.get("Prox_Riverain", 0) or 0)

        inputs = pd.DataFrame([inputs_dict])
        inputs = create_features(inputs, region_key, is_training=False)
        X_transformed = pipe.named_steps["preproc"].transform(inputs)

        # Background
        df = pd.read_csv(config["data_path"])
        df = create_features(df, region_key, is_training=True).dropna(subset=['Prix_de_vente'])
        background_sample = df[features].sample(min(100, len(df)), random_state=42)
        X_background = pipe.named_steps["preproc"].transform(background_sample)

        # SHAP
        model = pipe.named_steps["model"]
        explainer = shap.TreeExplainer(model, data=X_background)
        shap_values = explainer.shap_values(X_transformed)  # (1, n_features)

        shap_results = {}
        for i, feature in enumerate(features):
            sv = float(shap_values[0][i])

            if feature == "Aire_Batiment":
                per_m2 = (sv / aire_batiment) if aire_batiment > 0 else 0.0
                shap_results[feature] = {
                    "shap_value": sv,
                    "description": f"Each extra 10 m¬≤ adds ~${per_m2 * 10:,.0f}",
                    "label": "Building Efficiency"
                }

            elif feature == "Age":
                modern_threshold = 20
                if age < modern_threshold:
                    shap_results[feature] = {
                        "shap_value": sv,
                        "description": f"Modern building (Age < {modern_threshold}) adds ${sv:,.0f}",
                        "label": "Condition"
                    }
                else:
                    shap_results[feature] = {
                        "shap_value": sv,
                        "description": f"Older building (Age ‚â• {modern_threshold}) reduces value by ${abs(sv):,.0f}",
                        "label": "Condition"
                    }

            elif feature == "Prox_Riverain":
                if prox_riverain == 1:
                    shap_results[feature] = {
                        "shap_value": sv,
                        "description": f"Waterfront location adds ${sv:,.0f}",
                        "label": "Premium Location"
                    }
                else:
                    shap_results[feature] = {
                        "shap_value": sv,
                        "description": f"Non-waterfront location: ${sv:,.0f} impact",
                        "label": "Premium Location"
                    }

            elif feature == "Etage":
                if etage > 1:
                    shap_results[feature] = {
                        "shap_value": sv,
                        "description": f"Multi-floor ({int(etage)} floors) adds ${sv:,.0f}",
                        "label": "Floor Level"
                    }
                else:
                    shap_results[feature] = {
                        "shap_value": sv,
                        "description": f"Single floor: ${sv:,.0f} impact",
                        "label": "Floor Level"
                    }

            elif feature == "Aire_Lot":
                per_m2 = (sv / aire_lot) if aire_lot > 0 else 0.0
                shap_results[feature] = {
                    "shap_value": sv,
                    "description": f"Each extra 10 m¬≤ adds ~${per_m2 * 10:,.0f}",
                    "label": "Lot Size"
                }

            # For other features (e.g., PMR category/park/metro), you can add cases later.
        return shap_results

    except Exception as e:
        st.warning(f"Could not compute SHAP values: {e}")
        return None




def main():
    # Header
    st.markdown('<h1 class="main-header">üè† Property Valuation System</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; font-size: 1.2rem; color: #666;">Multi-Region Gradient Boosting with Quantile Loss for Accurate Property Valuations</p>', unsafe_allow_html=True)
    
    # Region selector
    st.sidebar.title("Region Selection")
    region_key = st.sidebar.selectbox(
        "Select Region:",
        ["BDF", "PMR", "Ste-Rose"],
        index=0,
        format_func=lambda x: REGION_CONFIG[x]["name"]
    )
    
    # Show selected region info
    selected_region = REGION_CONFIG[region_key]
    st.sidebar.info(f"**Selected:** {selected_region['name']}")
    
    # Sidebar navigation
    st.sidebar.title("Navigation")
    page = st.sidebar.selectbox(
        "Choose a page:",
        ["Home", "Model Performance", "Property Valuation", "About"]
    )
    
    if page == "Home":
        st.markdown("""
        ## Welcome to the Advanced Property Valuation System
        
        This system uses **Gradient Boosting with Quantile Loss** to provide not just point estimates, but also confidence intervals for property valuations.
        
        ### Key Features:
        - **Gradient Boosting with Quantile Loss**: Provides 5th, 50th, and 95th percentile predictions
        - **Confidence Intervals**: Understand the uncertainty in your valuations
        - **Advanced Features**: Raw attributes for more relevant insights
        
        ### How to Use:
        1. **Train the Model**: Go to "Model Performance" to train the quantile models
        2. **Get Valuations**: Use "Property Valuation" for custom estimates
        """)
        
                # Replace the whole "Check model status" block with this:
        model_exists = model_path_for(region_key, 0.5).exists()
        if model_exists:
            st.markdown('<div class="success-box">', unsafe_allow_html=True)
            st.markdown("‚úÖ **Model Status**: Quantile models are trained and ready to use!")
            st.markdown("</div>", unsafe_allow_html=True)
        else:
            st.markdown('<div class="warning-box">', unsafe_allow_html=True)
            st.markdown("‚ö†Ô∏è **Model Status**: Models need to be trained for this region. Go to 'Model Performance' to train the models.")
            st.markdown("</div>", unsafe_allow_html=True)

        # Quick stats
        col1, col2, col3 = st.columns(3)

        
        with col1:
            st.metric("Model Type", "Gradient Boosting with Quantile Loss", "Advanced")
        
        with col2:
            st.metric("Prediction Range", "5th - 95th percentile", "Confidence Intervals")
        
        with col3:
            st.metric("Features", "5 Raw Attributes", "Raw attributes for more relevant insights")
    
    elif page == "Property Valuation":
        st.header(f"üè† Property Valuation - {selected_region['name']}")
        
        # Check if models exist for this region
        model_path = model_path_for(region_key, 0.5)
        if not model_path.exists():
            st.error(f"‚ö†Ô∏è Models for {selected_region['name']} need to be trained first. Please go to 'Model Performance' to train the models.")
            return
        
        # Dynamic input fields based on region
        with st.form("valuation_form"):
            st.subheader("Enter Property Details")
            
            # Get region configuration
            config = REGION_CONFIG[region_key]
            feature_cols = config["feature_cols"]
            num_cols = config["num_cols"]
            cat_cols = config["cat_cols"]
            
            # Create input fields dynamically
            input_values = {}
            
            # Split into two columns
            col1, col2 = st.columns(2)
            
            with col1:
                for i, feature in enumerate(feature_cols[:len(feature_cols)//2 + 1]):
                    if feature == "Etage":
                        input_values[feature] = st.number_input("Floor", min_value=1, max_value=20, value=2)
                    elif feature == "Age":
                        input_values[feature] = st.number_input("Building Age (years)", min_value=0, max_value=100, value=15)
                    elif feature == "Aire_Batiment":
                        input_values[feature] = st.number_input("Building Area (m¬≤)", min_value=20.0, max_value=1000.0, value=120.0, step=10.0)
                    elif feature == "Aire_Lot":
                        input_values[feature] = st.number_input("Lot Area (m¬≤)", min_value=50.0, max_value=2000.0, value=300.0, step=50.0)
                    elif feature == "Prox_Riverain":
                        input_values[feature] = st.selectbox("Waterfront Proximity", options=[0, 1], format_func=lambda x: "No" if x == 0 else "Yes")
                    elif feature == "Taxes_annuelles":
                        input_values[feature] = st.number_input("Annual Taxes ($)", min_value=0.0, max_value=50000.0, value=3000.0, step=100.0)
                    elif feature == "Category":
                        # Load unique categories from PMR dataset
                        try:
                            df_pmr = pd.read_csv(config["data_path"])
                            categories = df_pmr["Category"].unique().tolist()
                            input_values[feature] = st.selectbox("Category", options=categories)
                        except:
                            input_values[feature] = st.text_input("Category", value="Residential")
                    elif feature in ["Near_A_Park", "Near_Metro_Station"]:
                        input_values[feature] = st.selectbox(feature.replace("_", " "), options=[0, 1], format_func=lambda x: "No" if x == 0 else "Yes")
            
            with col2:
                for feature in feature_cols[len(feature_cols)//2 + 1:]:
                    if feature == "Etage":
                        input_values[feature] = st.number_input("Floor", min_value=1, max_value=20, value=2)
                    elif feature == "Age":
                        input_values[feature] = st.number_input("Building Age (years)", min_value=0, max_value=100, value=15)
                    elif feature == "Aire_Batiment":
                        input_values[feature] = st.number_input("Building Area (m¬≤)", min_value=20.0, max_value=1000.0, value=120.0, step=10.0)
                    elif feature == "Aire_Lot":
                        input_values[feature] = st.number_input("Lot Area (m¬≤)", min_value=50.0, max_value=2000.0, value=300.0, step=50.0)
                    elif feature == "Prox_Riverain":
                        input_values[feature] = st.selectbox("Waterfront Proximity", options=[0, 1], format_func=lambda x: "No" if x == 0 else "Yes")
                    elif feature == "Taxes_annuelles":
                        input_values[feature] = st.number_input("Annual Taxes ($)", min_value=0.0, max_value=50000.0, value=3000.0, step=100.0)
                    elif feature == "Category":
                        try:
                            df_pmr = pd.read_csv(config["data_path"])
                            categories = df_pmr["Category"].unique().tolist()
                            input_values[feature] = st.selectbox("Category", options=categories)
                        except:
                            input_values[feature] = st.text_input("Category", value="Residential")
                    elif feature in ["Near_A_Park", "Near_Metro_Station"]:
                        input_values[feature] = st.selectbox(feature.replace("_", " "), options=[0, 1], format_func=lambda x: "No" if x == 0 else "Yes")
            
            submitted = st.form_submit_button("Get Valuation")
            
            if submitted:
                try:
                    # Validate inputs
                    if "Aire_Batiment" in input_values and input_values["Aire_Batiment"] <= 0:
                        st.error("Please enter a valid building area.")
                        return
                    
                    # Make prediction with region-aware function
                    low, median, high = predict_with_models(region_key, **input_values)
                    
                    if low is None or median is None or high is None:
                        st.error("Prediction failed. Please check if models are trained.")
                        return
                    
                    # Get MAE for display
                    mae = get_model_mae(region_key)
                    
                    # Calculate ratio for KPI
                    ratio_result = find_nearest_neighbors_and_calculate_ratio(
                        region_key=region_key, predicted_value=median, **input_values
                    )
                    
                    # Display results
                    st.markdown('<div class="prediction-card">', unsafe_allow_html=True)
                    st.markdown(f"## Estimated Property Value")
                    st.markdown(f"# ${median:,.0f}")
                    if mae is not None:
                        st.markdown(f"*MAE: ${mae:,.0f}*")
                    st.markdown("</div>", unsafe_allow_html=True)
                    
                    # Add explanation about the model
                    st.info("This model uses Gradient Boosting with Quantile Loss, a machine learning method that provides price predictions along with confidence ranges.")
                    
                    # Quantile breakdown
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.markdown('<div class="quantile-card">', unsafe_allow_html=True)
                        st.markdown("### Lower Bound")
                        st.markdown(f"## $ {low:,.0f}")
                        st.markdown("*5th percentile*")
                        st.markdown("</div>", unsafe_allow_html=True)
                    
                    with col2:
                        st.markdown('<div class="quantile-card">', unsafe_allow_html=True)
                        st.markdown("### Median")
                        st.markdown(f"## $ {median:,.0f}")
                        st.markdown("*50th percentile*")
                        st.markdown("</div>", unsafe_allow_html=True)
                    
                    with col3:
                        st.markdown('<div class="quantile-card">', unsafe_allow_html=True)
                        st.markdown("### Upper Bound")
                        st.markdown(f"## $ {high:,.0f}")
                        st.markdown("*95th percentile*")
                        st.markdown("</div>", unsafe_allow_html=True)
                    
                    # SHAP Waterfall Chart
                    st.subheader("Price Construction (SHAP Waterfall)")
                    fig = create_shap_waterfall_chart(region_key=region_key, predicted_value=median, **input_values)
                    if fig:
                        st.plotly_chart(fig, use_container_width=True)
                        st.info("üí° **Price Construction**: This chart starts from the dataset's average property value and shows how each attribute contributes positively or negatively to the final predicted price.")
                    else:
                        st.warning("Could not generate the price construction chart.")
                    
                    # Additional metrics
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        price_per_m2 = median / float(input_values.get("Aire_Batiment", 0) or 1)

                        st.metric("Price per m¬≤", f"${price_per_m2:,.0f}")
                    
                    with col2:
                        if ratio_result:
                            st.metric("Ratio Valuation vs Price", f"{ratio_result['ratio']:.2f}%")
                            st.caption(f"Actual Price: ${ratio_result['actual_price']:,.0f}")
                            if ratio_result['used_averaging']:
                                st.caption(f"Average of {ratio_result['k']} similar properties")
                        else:
                            st.metric("Ratio Valuation vs Price", "N/A")
                    
                    with col3:
                        if mae is not None:
                            st.metric("Mean Absolute Error", f"${mae:,.0f}", "Average prediction error")
                        else:
                            st.metric("Mean Absolute Error", "N/A", "Not available")
                    
                    # Add helper text for the ratio KPI
                    if ratio_result:
                        st.info("üí° **Ratio Valuation vs Price**: This ratio compares the model's valuation to the closest known property or an average of similar properties in the dataset. 100% = perfect match, >100% = overestimation, <100% = underestimation.")

                    
                    # SHAP-based Property Analysis
                    st.subheader("Property Analysis (SHAP values)")
                    
                    # Compute SHAP values for this specific property
                    # SHAP values explain how each feature contributes to the prediction in dollars
                    # Positive values increase the predicted price, negative values decrease it
                    shap_results = compute_shap_values(region_key=region_key, **input_values)
                    
                    if shap_results:
                        st.markdown('<div class="compact-info">üí° <strong>SHAP Analysis</strong>: Each value shows how much each feature contributes to the predicted price in dollars.</div>', unsafe_allow_html=True)
                        
                        # Maintain logical grouping but render as compact horizontal cards
                        shap_sections = {
                            "Building Efficiency": ["Aire_Batiment"],
                            "Condition": ["Age"],
                            "Premium Location": ["Prox_Riverain"],
                            "Floor Level": ["Etage"],
                            "Lot Size": ["Aire_Lot"]
                        }
                        
                        cards_html_parts = []
                        for section_name, features in shap_sections.items():
                            for feature in features:
                                if feature in shap_results:
                                    result = shap_results[feature]
                                    shap_val = float(result["shap_value"]) if hasattr(result["shap_value"], "__float__") else result["shap_value"]
                                    desc = result["description"]
                                    label = result.get("label", feature)
                                    is_positive = shap_val >= 0
                                    icon = "üìà" if is_positive else "üìâ"
                                    sign_class = "positive" if is_positive else "negative"
                                    
                                    # Map section names to theme-consistent CSS classes
                                    section_class_map = {
                                        "Building Efficiency": "building-efficiency",
                                        "Condition": "condition", 
                                        "Premium Location": "premium-location",
                                        "Floor Level": "floor-level",
                                        "Lot Size": "lot-size"
                                    }
                                    theme_class = section_class_map.get(section_name, "building-efficiency")
                                    
                                    cards_html_parts.append(
                                        f"<div class='shap-card {theme_class} {sign_class}'>"
                                        f"<h5>{label}</h5>"
                                        f"<div class='value'>{icon} ${shap_val:,.0f}</div>"
                                        f"<p class='desc'>{desc}</p>"
                                        f"</div>"
                                    )

                        if cards_html_parts:
                            cards_html = "".join(cards_html_parts)
                            st.markdown(f"<div class='shap-card-grid'>{cards_html}</div>", unsafe_allow_html=True)
                    else:
                        st.warning("SHAP analysis could not be computed for this property.")
                    
                except Exception as e:
                    st.error(f"An error occurred: {e}")
    

    
    elif page == "Model Performance":
        st.header(f"üìà Model Training & Performance - {selected_region['name']}")
        
        # Check if models exist for this region
        model_path = model_path_for(region_key, 0.5)
        
        if st.button(f"üöÄ Train Quantile Models for {selected_region['name']}", type="primary"):
            try:
                with st.spinner(f"Training models for {selected_region['name']}..."):
                    metrics = train_quantile_models(region_key)
                
                st.success(f"‚úÖ Models for {selected_region['name']} trained and saved successfully!")
                
                # Display metrics
                if "MAE" in metrics:
                    st.metric("Mean Absolute Error", f"${metrics['MAE']:,.0f}")
                    st.markdown("*Average prediction error*")
                
                if "R2" in metrics:
                    st.metric("R¬≤ Score", f"{metrics['R2']:.3f}")
                    st.markdown("*Model fit quality*")
                
                # Model info
                st.subheader("Model Information")
                config = REGION_CONFIG[region_key]
                st.markdown(f"""
                **Gradient Boosting with Quantile Loss Models Trained for {selected_region['name']}:**
                - **5th percentile model**: Lower bound predictions
                - **50th percentile model**: Median predictions  
                - **95th percentile model**: Upper bound predictions
                
                **Features Used:**
                - {', '.join(config['feature_cols'])}
                - Raw property attributes from the dataset
                - Full sklearn Pipeline with preprocessing
                """)
                
                if show_debug:
                    st.info(f"**Debug Info:** Models saved to {MODELS_DIR}")
                
            except Exception as e:
                st.error(f"‚ùå Training failed for {selected_region['name']}: {e}")
        
        elif model_path.exists():
            st.success(f"‚úÖ Models for {selected_region['name']} are already trained and ready to use!")
            
            # Load and display model info
            try:
                data = joblib.load(model_path)
                st.info("**Model Details:**")
                st.write(f"- Features: {len(data['features'])} attributes")
                st.write(f"- Region: {data.get('region', 'Unknown')}")
                st.write("- Algorithm: Gradient Boosting with Quantile Loss")
                st.write("- Pipeline: Full sklearn Pipeline with preprocessing")
                
                if "mae" in data:
                    st.write(f"- MAE: ${data['mae']:,.0f}")
                
                if show_debug:
                    st.info(f"**Debug Info:** Model loaded from {model_path}")
                    st.write(f"Pipeline steps: {list(data['pipeline'].named_steps.keys())}")
                    
            except Exception as e:
                st.warning(f"Model files exist but may be corrupted: {e}. Please retrain.")
        else:
            st.info(f"‚ÑπÔ∏è No models found for {selected_region['name']}. Click the button above to train models.")
    
    elif page == "About":
        st.header("‚ÑπÔ∏è About")
        # Note: Using "Gradient Boosting with Quantile Loss" instead of "Quantile Regression" 
        # because quantile regression typically refers to linear methods, while this app uses GradientBoostingRegressor with quantile loss
        
        st.markdown("""
        ## Advanced Property Valuation System
        
        This system uses **Gradient Boosting with Quantile Loss** to provide comprehensive property valuations with confidence intervals.
        
        ### Technology Stack:
        - **Python**: Core programming language
        - **Streamlit**: Web application framework
        - **Scikit-learn**: Machine learning library with Gradient Boosting and quantile loss
        - **Plotly**: Interactive visualizations
        - **Pandas & NumPy**: Data manipulation
        
        ### Model Architecture:
        The system uses **Gradient Boosting with Quantile Loss**:
        - **5th percentile model**: Lower bound predictions (conservative estimate)
        - **50th percentile model**: Median predictions (most likely value)
        - **95th percentile model**: Upper bound predictions (optimistic estimate)
        
        ### Features Used:
        - Floor level
        - Building age (years)
        - Building area (m¬≤)
        - Lot area (m¬≤)
        - Waterfront proximity (Yes/No)
        - Raw property attributes from the dataset
        
        ### Advantages of Gradient Boosting with Quantile Loss:
        - **Uncertainty Quantification**: Provides confidence intervals
        - **Robust Predictions**: Less sensitive to outliers
        - **Risk Assessment**: Helps understand prediction uncertainty
        - **Better Decision Making**: Range of possible values instead of single point
        
        ### Data Source:
        The model is trained on real estate data from the Land Register of Quebec.
        
        ### Disclaimer
        
        This application does not replace the advice of a certified appraiser. However, it uses machine learning to predict the price of a property based on real property data. Statistical validity may vary from one model to another depending on the ML algorithm used.
        
        ---
        **Note**:  
        The model is evaluated internally using several statistical metrics (including R¬≤), but these are kept under the hood.  
        What you see in the app are the confidence intervals and error metrics (such as MAE), which are the most relevant for decision-making.
        """)

if __name__ == "__main__":
    main()
